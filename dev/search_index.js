var documenterSearchIndex = {"docs":
[{"location":"references/#references-section","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Modules = [HMatrices]","category":"page"},{"location":"references/#HMatrices.ALLOW_GETINDEX","page":"References","title":"HMatrices.ALLOW_GETINDEX","text":"const ALLOW_GETINDEX\n\nIf set to false, the getindex(H,i,j) method will throw an error on AbstractHMatrix.\n\n\n\n\n\n","category":"constant"},{"location":"references/#HMatrices.CACHED_PARTITIONS","page":"References","title":"HMatrices.CACHED_PARTITIONS","text":"const CACHED_PARTITIONS\n\nA WeakKeyDict mapping a hierarhical matrix to a Partition of itself. Used when computing e.g. the forward map (i.e. mul!) to avoid having to recompute the partition for each matrix/vector product.\n\n\n\n\n\n","category":"constant"},{"location":"references/#Base.Matrix-Tuple{HMatrix}","page":"References","title":"Base.Matrix","text":"Matrix(H::HMatrix;global_index=false)\n\nConvert H to a Matrix. If global_index=true, the entries are given in the global indexing system (see HMatrix for more information); otherwise the local indexing system induced by the row and columns trees are used (default).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.ACA","page":"References","title":"HMatrices.ACA","text":"struct ACA\n\nAdaptive cross approximation algorithm with full pivoting. This structure can be used to generate an RkMatrix from a matrix-like object M. The keywork arguments rtol, atol, and rank can be used to control the quality of the approximation. Note that because ACA uses full pivoting, the linear operator M has to be evaluated at every i,j.\n\nSee also: [PartialACA](@ref)\n\nExamples\n\nusing LinearAlgebra\nrtol = 1e-6\ncomp = ACA(;rtol)\nA = rand(10,2)\nB = rand(10,2)\nM = A*adjoint(B) # a low-rank matrix\nR = comp(M,:,:) # compress the entire matrix `M`\nnorm(Matrix(R) - M) < rtol*norm(M) # true\n\n# output\n\ntrue\n\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.AbstractCompressor","page":"References","title":"HMatrices.AbstractCompressor","text":"abstract type AbstractCompressor\n\nTypes used to compress matrices.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.AbstractKernelMatrix","page":"References","title":"HMatrices.AbstractKernelMatrix","text":"abstract type AbstractKernelMatrix{T} <: AbstractMatrix{T}\n\nInterface for abstract matrices represented through a kernel function f, target elements X, and source elements Y. The matrix entry i,j is given by f(X[i],Y[j]). Concrete subtypes should implement at least\n\n`Base.getindex(K::AbstractKernelMatrix,i::Int,j::Int)`\n\nIf a more efficient implementation of getindex(K,I::UnitRange,I::UnitRange), getindex(K,I::UnitRange,j::Int) and getindex(adjoint(K),I::UnitRange,j::Int) is available (e.g. with SIMD vectorization), implementing such methods can improve the speed of assembling an HMatrix.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.DHMatrix","page":"References","title":"HMatrices.DHMatrix","text":"mutable struct DHMatrix{R,T} <: AbstractHMatrix{T}\n\nConcrete type representing a hierarchical matrix with data distributed amongst various workers. Its structure is very similar to HMatrix, except that the leaves store a RemoteHMatrix object.\n\nThe data on the leaves of a DHMatrix may live on a different worker, so calling fetch on them should be avoided whenever possible.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.DHMatrix-Union{Tuple{T}, Tuple{R}, Tuple{R, R}} where {R, T}","page":"References","title":"HMatrices.DHMatrix","text":"DHMatrix{T}(rowtree,coltree;partition_strategy=:distribute_columns)\n\nConstruct the block structure of a distributed hierarchical matrix covering rowtree and coltree. Returns a DHMatrix with leaves that are empty.\n\nThe partition_strategy keyword argument determines how to partition the blocks for distributed computing. Currently, the only available options is distribute_columns, which will partition the columns of the underlying matrix into floor(log2(nw)) parts, where nw is the number of workers available.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.HMatrix","page":"References","title":"HMatrices.HMatrix","text":"mutable struct HMatrix{R,T} <: AbstractHMatrix{T}\n\nA hierarchial matrix constructed from a rowtree and coltree of type R and holding elements of type T.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.HMatrix-Union{Tuple{T}, Tuple{R}, Tuple{R, R, Any}} where {R, T}","page":"References","title":"HMatrices.HMatrix","text":"HMatrix{T}(rowtree,coltree,adm)\n\nConstruct an empty HMatrix with rowtree and coltree using the admissibility condition adm. This function builds the skeleton for the hierarchical matrix, but does not compute data field in the blocks. See assemble_hmat for assembling a hierarhical matrix.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.HMulNode","page":"References","title":"HMatrices.HMulNode","text":"struct HMulNode{S,T} <: AbstractTree\n\nTree data structure representing the following computation:\n\n    C <-- C + a * ∑ᵢ Aᵢ * Bᵢ\n\nwhere C = target(node), Aᵢ,Bᵢ are pairs stored in sources(node), and a is stored in the multiplier field.\n\nThis structure is used to group the operations required when multiplying hierarchical matrices so that they can later be executed in a way that minimizes recompression of intermediate computations.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.KernelMatrix","page":"References","title":"HMatrices.KernelMatrix","text":"KernelMatrix{Tf,Tx,Ty,T} <:: AbstractKernelMatrix{T}\n\nGeneric kernel matrix representing a kernel function acting on two sets of elements. If K is a KernelMatrix, then K[i,j] = f(X[i],Y[j]) where f::Tf=kernel(K), X::Tx=rowelements(K) and Y::Ty=colelements(K).\n\nExamples\n\nX = rand(Geometry.Point2D,100)\nY = rand(Geometry.Point2D,100)\nK = KernelMatrix(X,Y) do x,y\n    sum(x+y)\nend\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.PartialACA","page":"References","title":"HMatrices.PartialACA","text":"struct PartialACA\n\nAdaptive cross approximation algorithm with partial pivoting. This structure can be used to generate an RkMatrix from a matrix-like object M as follows:\n\nusing LinearAlgebra\nrtol = 1e-6\ncomp = PartialACA(;rtol)\nA = rand(10,2)\nB = rand(10,2)\nM = A*adjoint(B) # a low-rank matrix\nR = comp(M) # compress the entire matrix `M`\nnorm(Matrix(R) - M) < rtol*norm(M) # true\n\n# output\n\ntrue\n\n\nBecause it uses partial pivoting, the linear operator does not have to be evaluated at every i,j. This is usually much faster than ACA, but due to the pivoting strategy the algorithm may fail in special cases, even when the underlying linear operator is of low rank.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.Partition","page":"References","title":"HMatrices.Partition","text":"struct Partition{T<:HMatrix}\n\nA partition of the leaves of an HMatrix. Used to perform threaded hierarchical multiplication.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.PermutedMatrix","page":"References","title":"HMatrices.PermutedMatrix","text":"PermutedMatrix{K,T} <: AbstractMatrix{T}\n\nStructured used to reprensent the permutation of a matrix-like object. The original matrix is stored in the data::K field, and the permutations are stored in rowperm and colperm.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.RemoteHMatrix","page":"References","title":"HMatrices.RemoteHMatrix","text":"struct RemoteHMatrix{S,T}\n\nA light wrapper for a Future storing an HMatrix.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.RkMatrix","page":"References","title":"HMatrices.RkMatrix","text":"mutable struct RkMatrix{T}\n\nRepresentation of a rank r matrix M in outer product format M = A*adjoint(B) where A has size m × r and B has size n × r.\n\nThe internal representation stores A and B, but R.Bt or R.At can be used to get the respective adjoints.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.RkMatrix-Tuple{LinearAlgebra.SVD}","page":"References","title":"HMatrices.RkMatrix","text":"RkMatrix(F::SVD)\n\nConstruct an RkMatrix from an SVD factorization.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.RkMatrix-Tuple{Matrix}","page":"References","title":"HMatrices.RkMatrix","text":"RkMatrix(M::Matrix)\n\nConstruct an RkMatrix from a Matrix by passing through the full svd of M.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.RkMatrix-Union{Tuple{V}, Tuple{Vector{V}, Vector{V}}} where V<:(AbstractVector)","page":"References","title":"HMatrices.RkMatrix","text":"RkMatrix(A::Vector{<:Vector},B::Vector{<:Vector})\n\nConstruct an RkMatrix from a vector of vectors. Assumes that length(A) == length(B), which determines the rank, and that all vectors in A (resp. B) have the same length m (resp. n).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.StrongAdmissibilityStd","page":"References","title":"HMatrices.StrongAdmissibilityStd","text":"struct StrongAdmissibilityStd\n\nTwo blocks are admissible under this condition if the minimum of their diameter is smaller than eta times the distance between them, where eta::Float64 is a parameter.\n\nUsage:\n\nadm = StrongAdmissibilityStd(;eta=2.0)\nadm(Xnode,Ynode)\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.TSVD","page":"References","title":"HMatrices.TSVD","text":"struct TSVD\n\nCompression algorithm based on a posteriori truncation of an SVD. This is the optimal approximation in Frobenius norm; however, it also tends to be very expensive and thus should be used mostly for \"small\" matrices.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.WeakAdmissibilityStd","page":"References","title":"HMatrices.WeakAdmissibilityStd","text":"struct WeakAdmissibilityStd\n\nTwo blocks are admissible under this condition if the distance between them is positive.\n\n\n\n\n\n","category":"type"},{"location":"references/#Base.hcat-Union{Tuple{T}, Tuple{HMatrices.RkMatrix{T}, HMatrices.RkMatrix{T}}} where T","page":"References","title":"Base.hcat","text":"hcat(M1::RkMatrix,M2::RkMatrix)\n\nConcatenated M1 and M2 horizontally to produce a new RkMatrix of rank rank(M1)+rank(M2).\n\n\n\n\n\n","category":"method"},{"location":"references/#Base.vcat-Union{Tuple{T}, Tuple{HMatrices.RkMatrix{T}, HMatrices.RkMatrix{T}}} where T","page":"References","title":"Base.vcat","text":"vcat(M1::RkMatrix,M2::RkMatrix)\n\nConcatenated M1 and M2 vertically to produce a new RkMatrix of rank rank(M1)+rank(M2)\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._aca_full!-NTuple{4, Any}","page":"References","title":"HMatrices._aca_full!","text":"_aca_full!(M,atol,rmax,rtol)\n\nInternal function implementing the adaptive cross-approximation algorithm with full pivoting. The matrix M is modified in place. The returned RkMatrix has rank at most rmax, and is expected to satisfy |M - R| < max(atol,rtol*|M|).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._aca_full_pivot-Tuple{Any}","page":"References","title":"HMatrices._aca_full_pivot","text":"_aca_full_pivot(M)\n\nFind the index of the element x ∈ M maximizing its smallest singular value. This is equivalent to minimizing the spectral norm of the inverse of x.\n\nWhen x is a scalar, this is simply the element with largest absolute value.\n\nSee also: _aca_partial_pivot.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._aca_partial","page":"References","title":"HMatrices._aca_partial","text":"_aca_partial(K,irange,jrange,atol,rmax,rtol,istart=1)\n\nInternal function implementing the adaptive cross-approximation algorithm with partial pivoting. The returned R::RkMatrix provides an approximation to K[irange,jrange] which has either rank is expected to satisfy|M - R| < max(atol,rtol*|M|)`, but this inequality may fail to hold due to the various errors involved in estimating the error and |M|.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices._aca_partial_pivot-Tuple{Any, Any}","page":"References","title":"HMatrices._aca_partial_pivot","text":"_aca_partial_pivot(v,I)\n\nFind in the valid set I the index of the element x ∈ v maximizing its smallest singular value. This is equivalent to minimizing the spectral norm of the inverse of x.\n\nWhen x is a scalar, this is simply the element with largest absolute value.\n\nThis general implementation should work for both scalar as well as tensor-valued kernels; see (https://www.sciencedirect.com/science/article/pii/S0021999117306721)[https://www.sciencedirect.com/science/article/pii/S0021999117306721] for more details.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._assemble_cpu!-Tuple{Any, Any, Any}","page":"References","title":"HMatrices._assemble_cpu!","text":"_assemble_cpu!(hmat::HMatrix,K,comp)\n\nAssemble data on the leaves of hmat. The admissible leaves are compressed using the compressor comp. This function assumes the structure of hmat has already been intialized, and therefore should not be called directly. See HMatrix information on constructors.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._assemble_hmat_distributed-Tuple{Any, Any, Any}","page":"References","title":"HMatrices._assemble_hmat_distributed","text":"_assemble_hmat_distributed(K,rtree,ctree;adm=StrongAdmissibilityStd(),comp=PartialACA();global_index=true,threads=false)\n\nInternal methods called after the DHMatrix structure has been initialized in order to construct the HMatrix on each of the leaves of the DHMatrix.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._assemble_threads!-Tuple{Any, Any, Any}","page":"References","title":"HMatrices._assemble_threads!","text":"_assemble_threads!(hmat::HMatrix,K,comp)\n\nLike _assemble_cpu!, but uses threads to assemble the leaves. Note that the threads are spanwned using Threads.@spawn, which means they are spawned on the same worker as the caller.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._build_block_structure!-Union{Tuple{T}, Tuple{R}, Tuple{Any, HMatrix{R, T}}} where {R, T}","page":"References","title":"HMatrices._build_block_structure!","text":"_build_block_structure!(adm_fun,current_node)\n\nRecursive constructor for HMatrix block structure. Should not be called directly.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._cost_gemv-Tuple{HMatrices.RkMatrix}","page":"References","title":"HMatrices._cost_gemv","text":"_cost_gemv(A::Union{Matrix,SubArray,Adjoint})\n\nA proxy for the computational cost of a matrix/vector product.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._hgemv_recursive!-Tuple{AbstractVector, Union{HMatrix, LinearAlgebra.Adjoint{<:Any, <:HMatrix}}, AbstractVector, Any}","page":"References","title":"HMatrices._hgemv_recursive!","text":"_hgemv_recursive!(C,A,B,offset)\n\nInternal function used to compute C[I] <-- C[I] + A*B[J] where I = rowrange(A) - offset[1] and J = rowrange(B) - offset[2].\n\nThe offset argument is used on the caller side to signal if the original hierarchical matrix had a pivot other than (1,1).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._update_frob_norm-Tuple{Any, Any, Any}","page":"References","title":"HMatrices._update_frob_norm","text":"_update_frob_norm(acc,A,B)\n\nGiven the Frobenius norm of Rₖ = A[1:end-1]*adjoint(B[1:end-1]) in acc, compute the Frobenius norm of Rₖ₊₁ = A*adjoint(B) efficiently.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.assemble_hmat-Tuple{Any, Any, Any}","page":"References","title":"HMatrices.assemble_hmat","text":"assemble_hmat(K,rowtree,coltree;adm=StrongAdmissibilityStd(),comp=PartialACA(),threads=true,distributed=false,global_index=true)\nassemble_hmat(K::KernelMatrix;threads=true,distributed=false,global_index=true,[rtol],[atol],[rank])\n\nMain routine for assembling a hierarchical matrix. The argument K represents the matrix to be approximated, rowtree and coltree are tree structure partitioning the row and column indices, respectively, adm can be called on a node of rowtree and a node of coltree to determine if the block is compressible, and comp is a function/functor which can compress admissible blocks.\n\nIt is assumed that K supports getindex(K,i,j), and that comp can be called as comp(K,irange::UnitRange,jrange::UnitRange) to produce a compressed version of K[irange,jrange] in the form of an RkMatrix.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.build_sequence_partition-NTuple{4, Any}","page":"References","title":"HMatrices.build_sequence_partition","text":"build_sequence_partition(seq,nq,cost,nmax)\n\nPartition the sequence seq into nq contiguous subsequences with a maximum of cost of nmax per set. Note that if nmax is too small, this may not be possible (see has_partition).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.compress!-Tuple{HMatrices.RkMatrix, TSVD}","page":"References","title":"HMatrices.compress!","text":"compress!(M::RkMatrix,tsvd::TSVD)\n\nRecompress the matrix R using a truncated svd of R. The implementation uses the qr-svd strategy to efficiently compute svd(R) when rank(R) ≪ min(size(R)).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.compress!-Tuple{Matrix, TSVD}","page":"References","title":"HMatrices.compress!","text":"compress!(M::Matrix,tsvd::TSVD)\n\nRecompress the matrix M using a truncated svd and output an RkMatrix. The data in M is invalidated in the process.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.compression_ratio-Tuple{HMatrices.RkMatrix}","page":"References","title":"HMatrices.compression_ratio","text":"compression_ratio(R::RkMatrix)\n\nThe ratio of the uncompressed size of R to its compressed size in outer product format.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.compression_ratio-Tuple{HMatrix}","page":"References","title":"HMatrices.compression_ratio","text":"compression_ratio(H::HMatrix)\n\nThe ratio of the uncompressed size of H to its compressed size.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.disable_getindex-Tuple{}","page":"References","title":"HMatrices.disable_getindex","text":"disable_getindex()\n\nCall this function to disable the getindex method on AbstractHMatrix. This is useful to avoid performance pitfalls associated with linear algebra methods falling back to a generic implementation which uses the getindex method. Calling getindex(H,i,j) will error after calling this function.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.enable_getindex-Tuple{}","page":"References","title":"HMatrices.enable_getindex","text":"enable_getindex()\n\nThe opposite of disable_getindex.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.find_optimal_cost","page":"References","title":"HMatrices.find_optimal_cost","text":"find_optimal_cost(seq,nq,cost,tol)\n\nFind an approximation to the cost of an optimal partitioning of seq into nq contiguous segments. The optimal cost is the smallest number cmax such that has_partition(seq,nq,cost,cmax) returns true.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.find_optimal_partition","page":"References","title":"HMatrices.find_optimal_partition","text":"find_optimal_partition(seq,nq,cost,tol=1)\n\nFind an approximation to the optimal partition seq into nq contiguous segments according to the cost function. The optimal partition is the one which minimizes the maximum cost over all possible partitions of seq into nq segments.\n\nThe generated partition is optimal up to a tolerance tol; for integer valued cost, setting tol=1 means the partition is optimal.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.flush_to_children!-Tuple{HMatrix}","page":"References","title":"HMatrices.flush_to_children!","text":"flush_to_children!(H::HMatrix,compressor)\n\nTransfer the blocks data to its children. At the end, set H.data to nothing.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.flush_to_leaves!-Tuple{HMatrix}","page":"References","title":"HMatrices.flush_to_leaves!","text":"flush_to_leaves!(H::HMatrix,compressor)\n\nTransfer the blocks data to its leaves. At the end, set H.data to nothing.\n\nSee also: flush_to_children!\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.getcol!-Tuple{Any, HMatrices.RkMatrix, Int64}","page":"References","title":"HMatrices.getcol!","text":"getcol!(col,M::AbstractMatrix,j)\n\nFill the entries of col with column j of M.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.getcol-Tuple{HMatrices.RkMatrix, Int64}","page":"References","title":"HMatrices.getcol","text":"getcol(M::AbstractMatrix,j)\n\nReturn a vector containing the j-th column of M.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.has_partition","page":"References","title":"HMatrices.has_partition","text":"has_partition(v,np,cost,cmax)\n\nGiven a vector v, determine whether or not a partition into np segments is possible where the cost of each partition does not exceed cmax.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.hilbert_cartesian_to_linear-Tuple{Integer, Any, Any}","page":"References","title":"HMatrices.hilbert_cartesian_to_linear","text":"hilbert_cartesian_to_linear(n,x,y)\n\nConvert the cartesian indices x,y into a linear index d using a hilbert curve of order n. The coordinates x,y range from 0 to n-1, and the output d ranges from 0 to n^2-1.\n\nSee https://en.wikipedia.org/wiki/Hilbert_curve.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.hilbert_linear_to_cartesian-Tuple{Integer, Any}","page":"References","title":"HMatrices.hilbert_linear_to_cartesian","text":"hilbert_linear_to_cartesian(n,d)\n\nConvert the linear index 0 ≤ d ≤ n^2-1 into the cartesian coordinates 0 ≤ x < n-1 and 0 ≤ y ≤ n-1 on the Hilbert curve of order n.\n\nSee https://en.wikipedia.org/wiki/Hilbert_curve.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.hilbert_partition","page":"References","title":"HMatrices.hilbert_partition","text":"hilbert_partition(H::HMatrix,np,cost)\n\nPartiotion the leaves of H into np sequences of approximate equal cost (as determined by the cost function) while also trying to maximize the locality of each partition.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.hmul!-Tuple{HMatrix, HMatrix, HMatrix, Any, Any, Any}","page":"References","title":"HMatrices.hmul!","text":"hmul!(C::HMatrix,A::HMatrix,B::HMatrix,a,b,compressor)\n\nSimilar to mul! : compute C <-- A*B*a + B*b, where A,B,C are hierarchical matrices and compressor is a function/functor used in the intermediate stages of the multiplication to avoid growring the rank of admissible blocks after addition is performed.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.isclean-Tuple{HMatrix}","page":"References","title":"HMatrices.isclean","text":"isclean(H::HMatrix)\n\nReturn true if all leaves of H have data, and if the leaves are the only nodes containing data. This is the normal state of an ℋ-matrix, but during intermediate stages of a computation data may be associated with non-leaf nodes for convenience.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.num_stored_elements-Tuple{HMatrices.RkMatrix}","page":"References","title":"HMatrices.num_stored_elements","text":"num_stored_elements(R::RkMatrix)\n\nThe number of entries stored in the representation. Note that this is not length(R).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.use_global_index-Tuple{}","page":"References","title":"HMatrices.use_global_index","text":"use_global_index()::Bool\n\nDefault choice of whether operations will use the global indexing system throughout the package.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.use_threads-Tuple{}","page":"References","title":"HMatrices.use_threads","text":"use_threads()::Bool\n\nDefault choice of whether threads will be used or not throught the package.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.axpy!-Tuple{Any, Matrix, HMatrices.RkMatrix}","page":"References","title":"LinearAlgebra.axpy!","text":"axpy!(a::Number,X::Union{Matrix,RkMatrix,HMatrix},Y::Union{Matrix,RkMatrix,HMatrix})\n\nPerform Y <-- a*X + Y in-place. Note that depending on the types of X and Y, this may require converting from/to different formats during intermediate calculations.\n\nIn the case where Y is an RkMatrix, the call axpy!(a,X,Y) should typically be followed by recompression stage to keep the rank of Y under control.\n\nIn the case where Y is an HMatrix, the call axpy!(a,X,Y) sums X to the data in the node Y (and not on the leaves). In case Y has no data, it will simply be assigned X. This means that after the call axpy(a,X,Y), the object Y is in a dirty state (see [isclean][@ref]) and usually a call to flush_to_leaves! or flush_to_children! follows.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.lu!-Tuple{HMatrix, Any}","page":"References","title":"LinearAlgebra.lu!","text":"lu!(M::HMatrix,comp)\n\nHierarhical LU facotrization of M, using comp to generate the compressed blocks during the multiplication routines.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.lu!-Tuple{HMatrix}","page":"References","title":"LinearAlgebra.lu!","text":"lu!(M::HMatrix;atol=0,rank=typemax(Int),rtol=atol>0 ||\nrank<typemax(Int) ? 0 : sqrt(eps(Float64)))\n\nHierarhical LU facotrization of M, using the PartialACA(;atol,rtol;rank) compressor.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.lu-Tuple{HMatrix, Vararg{Any}}","page":"References","title":"LinearAlgebra.lu","text":"lu(M::HMatrix,args...;kwargs...)\n\nHierarchical LU factorization. See lu! for the available options.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.mul!","page":"References","title":"LinearAlgebra.mul!","text":"mul!(y::AbstractVector,H::HMatrix,x::AbstractVector,a,b[;global_index,threads])\n\nPerform y <-- H*x*a + y*b in place.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.@hprofile-Tuple{Any}","page":"References","title":"HMatrices.@hprofile","text":"@hprofile\n\nA macro which\n\nresets the default TimerOutputs.get_defaulttimer to zero\nexecute the code block\nprint the profiling details\n\nThis is useful as a coarse-grained profiling strategy in HMatrices to get a rough idea of where time is spent. Note that this relies on TimerOutputs annotations manually inserted in the code.\n\n\n\n\n\n","category":"macro"},{"location":"benchs/#Benchmark-Report-for-*HMatrices*","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"","category":"section"},{"location":"benchs/#Job-Properties","page":"Benchmark Report for HMatrices","title":"Job Properties","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Time of benchmark: 11 Jun 2022 - 16:25\nPackage commit: 007341\nJulia commit: bf5349\nJulia command flags: -O3\nEnvironment variables: JULIA_NUM_THREADS => 4 OPEN_BLAS_NUM_THREADS => 1","category":"page"},{"location":"benchs/#Results","page":"Benchmark Report for HMatrices","title":"Results","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Below is a table of this job's results, obtained by running the benchmarks. The values listed in the ID column have the structure [parent_group, child_group, ..., key], and can be used to index into the BaseBenchmarks suite to retrieve the corresponding benchmarks. The percentages accompanying time and memory values in the below table are noise tolerances. The \"true\" time/memory value for a given benchmark is expected to fall within this percentage of the reported value. An empty cell means that the value was zero.","category":"page"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"ID time GC time memory allocations\n[\"Compressors\", \"ACA(0.0, 9223372036854775807, 1.0e-6)\"] 60.896 ms (5%) 16.845 ms 114.87 MiB (1%) 70\n[\"Compressors\", \"PartialACA(0.0, 9223372036854775807, 1.0e-6)\"] 215.466 μs (5%)  451.02 KiB (1%) 46\n[\"Compressors\", \"TSVD(0.0, 9223372036854775807, 1.0e-6)\"] 344.458 ms (5%) 8.569 ms 46.04 MiB (1%) 17\n[\"Helmholtz\", \"assemble cpu\"] 96.266 s (5%) 5.449 s 19.36 GiB (1%) 713824\n[\"Helmholtz\", \"assemble threads\"] 24.179 s (5%) 96.356 ms 19.36 GiB (1%) 736860\n[\"Helmholtz\", \"gemv cpu\"] 949.268 ms (5%)  3.05 MiB (1%) 6\n[\"Helmholtz\", \"gemv threads\"] 181.576 ms (5%)  9.16 MiB (1%) 55\n[\"Helmholtz\", \"lu\"] 66.773 s (5%) 763.845 ms 57.89 GiB (1%) 8146509\n[\"HelmholtzVec\", \"assemble cpu\"] 69.864 s (5%) 1.057 s 17.76 GiB (1%) 1092683\n[\"HelmholtzVec\", \"assemble threads\"] 20.562 s (5%) 310.865 ms 17.76 GiB (1%) 1116981\n[\"Laplace\", \"assemble cpu\"] 2.493 s (5%) 161.411 ms 4.52 GiB (1%) 315818\n[\"Laplace\", \"assemble threads\"] 604.592 ms (5%)  4.53 GiB (1%) 338851\n[\"Laplace\", \"gemv cpu\"] 272.634 ms (5%)  2.29 MiB (1%) 6\n[\"Laplace\", \"gemv threads\"] 76.399 ms (5%)  5.34 MiB (1%) 55\n[\"Laplace\", \"lu\"] 26.139 s (5%) 1.065 s 26.82 GiB (1%) 7637938\n[\"LaplaceVec\", \"assemble cpu\"] 2.300 s (5%)  4.11 GiB (1%) 343964\n[\"LaplaceVec\", \"assemble threads\"] 740.280 ms (5%)  4.11 GiB (1%) 366996","category":"page"},{"location":"benchs/#Benchmark-Group-List","page":"Benchmark Report for HMatrices","title":"Benchmark Group List","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Here's a list of all the benchmark groups executed by this job:","category":"page"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"[\"Compressors\"]\n[\"Helmholtz\"]\n[\"HelmholtzVec\"]\n[\"Laplace\"]\n[\"LaplaceVec\"]","category":"page"},{"location":"benchs/#Julia-versioninfo","page":"Benchmark Report for HMatrices","title":"Julia versioninfo","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Julia Version 1.7.2\nCommit bf53498635 (2022-02-06 15:21 UTC)\nPlatform Info:\n  OS: Linux (x86_64-pc-linux-gnu)\n      Ubuntu 20.04.4 LTS\n  uname: Linux 5.13.0-28-generic #31~20.04.1-Ubuntu SMP Wed Jan 19 14:08:10 UTC 2022 x86_64 x86_64\n  CPU: Intel(R) Xeon(R) W-2145 CPU @ 3.70GHz: \n                 speed         user         nice          sys         idle          irq\n       #1-16  1200 MHz    1843921 s       7865 s      84009 s  1574372975 s          0 s\n       \n  Memory: 251.4216766357422 GB (207606.51171875 MB free)\n  Uptime: 9.85318111e6 sec\n  Load Avg:  2.4  1.4  0.98\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-12.0.1 (ORCJIT, skylake-avx512)","category":"page"},{"location":"notebooks/#notebook-section","page":"Notebooks","title":"Notebooks","text":"","category":"section"},{"location":"notebooks/","page":"Notebooks","title":"Notebooks","text":"The following links provide some useful examples/tutorial in the format of notebooks. You need to download the notebook, and then open it with Pluto.jl in order to interact with it.","category":"page"},{"location":"notebooks/","page":"Notebooks","title":"Notebooks","text":"Vectorized kernels","category":"page"},{"location":"dhmatrix/#dhmatrix-section","page":"Distributed HMatrix","title":"Distributed hierarchical matrix","text":"","category":"section"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"warning: Warning\nThis is still an experimental feature!","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"When calling assemble_hmat, the keyword argument distributed can be set to true in order to generate a DHMatrix object. The main difference between an HMatrix and a DHMatrix is that the leaves of a DHMatrix represent a remote reference to an HMatrix possibly stored on a different worker. ","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"In order to use the distributed capabilities, you must first add the Distributed package and add some workers:","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"using Distributed\naddprocs(4)","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"You can then load the HMatrices package everywhere, and proceed as before:","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"@everywhere using HMatrices\nusing LinearAlgebra, StaticArrays\nconst Point3D = SVector{3,Float64}\nm = 10_000\nX = Y = [Point3D(sin(θ)cos(ϕ),sin(θ)*sin(ϕ),cos(θ)) for (θ,ϕ) in zip(π*rand(m),2π*rand(m))]\nconst μ = 5\nfunction G(x,y)\n    r = x-y\n    d = norm(r) + 1e-10\n    1/(8π*μ) * (1/d*I + r*transpose(r)/d^3)\nend\nK = KernelMatrix(G,X,Y)\nH = assemble_hmat(K;atol=1e-4,distributed=true,threads=false)","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"TODO: add an interactive notebook example ","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"CurrentModule = HMatrices","category":"page"},{"location":"#home-section","page":"Getting started","title":"HMatrices.jl","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"A package for assembling and factoring hierarchical matrices","category":"page"},{"location":"#Overview","page":"Getting started","title":"Overview","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"This package provides some functionality for assembling as well as for doing linear algebra with hierarchical matrices. The main structure exported is the HMatrix type, which can be used to efficiently approximate certain linear operators containing a hierarchical low-rank structure. Once assembled, a hierarchical matrix can be used to accelerate the solution of Ax=b in a variety of ways. Below you will find a quick introduction for how to assemble and utilize an HMatrix; see the References section for more information on the available methods and structures.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nAlthough hierarchical matrices have a broad range of application, this package focuses on their use to approximate integral operators arising in boundary integral equation (BIE) methods. As such, most of the API has been designed with BIEs in mind, and the examples that follow will focus on the compression of integral operators. Feel free to open an issue or reach out if you have an interesting application of hierarchical matrices in mind not covered by this package!","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Useful references\nThe notation and algorithms implemented were mostly drawn from the following references:Hackbusch, Wolfgang. Hierarchical matrices: algorithms and analysis. Vol. 49. Heidelberg: Springer, 2015.\nBebendorf, Mario. Hierarchical matrices. Springer Berlin Heidelberg, 2008.","category":"page"},{"location":"#assemble-generic-subsection","page":"Getting started","title":"Assembling an HMatrix","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"In order to assemble an HMatrix, you need the following (problem-specific) ingredients:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The matrix-like object K that you wish to compress\nA rowtree and coltree providing a hierarchical partition of the rows and columns of K\nAn admissibility condition for determining (a priory) whether a block given by a node in the rowtree and node in the coltree is compressible\nA function/functor to generate a low-rank approximation of compressible blocks","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"To illustrate how this is done for a concrete problem, consider two set of points X = left boldsymbolx_i right_i=1^m and Y =leftboldsymbolx_j right_j=1^n in mathbbR^3, and let K be a  m times n matrix with entries given by:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"  K_ij = G(boldsymbolx_iboldsymboly_j)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"for some kernel function G. To make things simple, we will take X and Y to be points distributed on a circle:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using Random # hide\nRandom.seed!(1) # hide\nusing HMatrices, LinearAlgebra, StaticArrays\nconst Point2D = SVector{2,Float64}\n\n# points on a circle\nm = n = 10_000\nX = Y = [Point2D(sin(i*2π/n),cos(i*2π/n)) for i in 0:n-1]\nnothing","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Next we will create the matrix-like structure to represent the object K. We will pick G to be the free-space Greens function for Laplace's equation in two-dimensions:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"struct LaplaceMatrix <: AbstractMatrix{Float64}\n  X::Vector{Point2D}  \n  Y::Vector{Point2D}\nend\n\nBase.getindex(K::LaplaceMatrix,i::Int,j::Int) = -1/2π*log(norm(K.X[i] - K.Y[j]) + 1e-10)\nBase.size(K::LaplaceMatrix) = length(K.X), length(K.Y)\n\n# create the abstract matrix\nK = LaplaceMatrix(X,Y)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The next step consists in partitioning the point clouds X and Y into a tree-like data structure so that blocks corresponding to well-separated points can be easily distinguished and compressed. The WavePropBase package provides the ClusterTree struct for this purpose (see its documentation for more details on available options):","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Xclt = Yclt = ClusterTree(X)\nnothing # hide","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The object Xclt represents a tree partition of the point cloud into axis-aligned bounding boxes.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The third requirement is an admissibilty condition to determine if the interaction between two clusters should be compressed. We will use the StrongAdmissibilityStd, which is appropriate for asymptotically smooth kernels such as the one considered:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"adm = StrongAdmissibilityStd()\nnothing # hide","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The final step is to provide a method to compress admissible blocks. Here we will use the PartialACA functor implementing an adaptive cross approximation with partial pivoting strategy:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"comp = PartialACA(;atol=1e-6)\nnothing # hide","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"With these ingredients at hand, we can assemble an approximation for K using","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"H = assemble_hmat(K,Xclt,Yclt;adm,comp,threads=false,distributed=false)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"important: Important\nThe assemble_hmat function is the main constructor exported by this package, so it is worth getting familiar with it and the various keyword arguments it accepts.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nReasonable defaults exist for the admissibility condition, cluster tree, and compressor when the kernel K is an AbstractKernelMatrix, so that the construction process is somewhat simpler than just presented in those cases. Manually constructing each ingredient, however, gives a level of control not available through the default constructors. See the Kernel matrices section for more details.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"You can now use H in lieu of K (as an approximation) for certain linear algebra operations, as shown next.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Disabling `getindex`\nAlthough the getindex(H,i::Int,j::Int) method is defined for an AbstractHMatrix, its use is mostly for display purposes in a REPL environment, and should be avoided in any linear algebra routine. To avoid the performance pitfalls related to methods falling back to the generic LinearAlgebra implementation of various algorithms (which make use getindex extensively), you can disable getindex on types defined in this package by calling HMatrices.disable_getindex. The consequence is that calling getindex(H,i,j) will throw an error. If a given operation is running unexpectedly slow, try disabling getindex to see if any part of the code is falling back to a generic implementation.","category":"page"},{"location":"#Matrix-vector-product-and-iterative-solvers","page":"Getting started","title":"Matrix vector product and iterative solvers","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"The simplest operation you can perform with an HMatrix is to multiply it by a vector:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"x = rand(n)\nnorm(H*x - K*x)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"More advanced options (such as choosing between a threaded or serial implementation) can be accessed by calling mul! directly:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"y = similar(x)\nmul!(y,H,x,1,0;threads=false)\nnorm(y - K*x)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The example below illustrates how to use the HMatrix H constructed above with the IterativeSolvers package:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using IterativeSolvers\nb = rand(m)\napprox = gmres!(y,H,b;abstol=1e-6)\nexact  = Matrix(K)\\b\nnorm(approx-exact)/norm(exact)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Internally, the hierarchical matrix H is stored as H = inv(Pr)*_H*Pc, where Pr and Pc are row and column permutation matrices induced by the clustering of the target and source points as ClusterTrees, respectively, and _H is the actual hierarchical matrix constructed. It is sometimes convenient to work directly with _H for performance reasons; for example, in the iterative solver above, you may want to permute rows and columns only once offline and perform the matrix multiplication with _H. The keyword argument global_index=false can be passed to perform the desired operations on _H instead, or you may overload the HMatrices.use_global_index method which will in turn change the default value of global_index throughout the package (but be careful to know what you are doing, as this may cause some unexpected results); similarly, you can overload HMatrices.use_threads to globally change whether threads are used by default. In the iterative example above, for instance, we may permute the vectors externally before and after (but not in each forward product) as follows:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"cperm  = HMatrices.colperm(H) # column permutation\nrperm  = HMatrices.rowperm(H) # row permutation\nbp     = b[cperm] \nHMatrices.use_global_index() = false # perform operations on the local indexing system\napprox = gmres!(y,H,bp;abstol=1e-6)\ninvpermute!(approx,rperm)\nHMatrices.use_global_index() = true # go back to default\nnorm(approx-exact)/norm(exact)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Problem size\nFor \"small\" problem sizes, the overhead associated with the more complex structure of an HMatrix will lead to computational times that are larger than the dense representation, even when the HMatrix occupies less memory. For large problem sizes, however, the loglinear complexity will yield significant gains in terms of memory and cpu time provided the underlying operator has a hierarchical low-rank structure.","category":"page"},{"location":"#Factorization-and-direct-solvers","page":"Getting started","title":"Factorization and direct solvers","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"Although the forward map illustrated in the example above suffices to solve the linear system Kx = b using an iterative solver, there are circumstances where a direct solver is desirable (because, e.g., the system is not well-conditioned or you wish to solve it for many right-hand-sides b). At present, the only available factorization is the hierarchical lu factorization of H, which can be accomplished as follows:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"F = lu(H;atol=1e-6)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Note that unliked the matrix-vector product, factoring H is not exact in the sense that lu(H) ≠ lu(Matrix(H)). The accuracy of the approximation can be controlled through the keyword arguments atol,rol and rank, which are used in the various intermediate truncations performed during the factorization. See lu for more details.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"important: Truncation error\nThe parameters atol and rtol are used to control the truncation of low-rank blocks adaptively using an estimate of the true error (in Frobenius norm). These local errors may accumulate after successive truncations, meaning that the global approximation error (in Frobenius norm) may be larger than the prescribed tolerance.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The returned object F is of the LU type, and efficient routines are provided to solve linear system using F:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"approx = F\\b\nnorm(approx-exact)/norm(exact)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Note that the error in solving the linear system may be significantly larger than the error in computing H*x due to the condition of the underlying operator.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nBecause factoring an HMatrix with a small error tolerance can be quite time-consuming, a hybrid strategy commonly employed consists of using a rough factorization (with e.g. large tolerance or a fixed rank) as a preconditioner to an iterative solver.","category":"page"},{"location":"#Other-kernels","page":"Getting started","title":"Other kernels","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"So far we focused on the (manual) compression of a simple kernel matrix where the entry (i,j) depended only on a function G and on point-clouds X and Y. There are many other interesting applications where the entry (i,j) requires more information, such as triangles, basis functions, or the normal vectors. To illustrate how the methods above could be adapted we consider now the construction of the double-layer kernel for Helmholtz equation. Our AbstractMatrix can then be defined as follows:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"struct LaplaceDoubleLayer <: AbstractMatrix{Float64}\n    X::Vector{Point2D}\n    Y::Vector{Point2D}\n    NY::Vector{Point2D} # normals at Y coordinate\nend\n\nfunction Base.getindex(K::LaplaceDoubleLayer,i::Int,j::Int) \n    r = K.X[i] - K.Y[j]\n    d = norm(r) + 1e-10\n    return (1 / (2π) / (d^2) * dot(r, K.NY[j]))\nend\nBase.size(K::LaplaceDoubleLayer) = length(K.X), length(K.Y)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"We can now simply instantiate a double-layer kernel, and compress it as before","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"# create the abstract matrix\nny = Y\nK = LaplaceDoubleLayer(X,Y,ny)\nH = assemble_hmat(K,Xclt,Yclt;adm,comp,threads=false,distributed=false)\n","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"With H assembled, everything else works exactly as before!","category":"page"},{"location":"#Index","page":"Getting started","title":"Index","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"","category":"page"},{"location":"kernelmatrix/#kernelmatrix-section","page":"Kernel matrices","title":"Kernel matrices","text":"","category":"section"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"While in the introduction we presented a somewhat general way to assemble an HMatrix, abstract matrices associated with an underlying kernel function are common enough in boundary integral equations that a special interface exists for facilitating their use.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"The AbstractKernelMatrix interface is used to represent matrices K with i,j entry given by f(X[i],Y[j]), where X=rowelements(K) and Y=colelements(K). The row and columns elements may be as simple as points in mathbbR^d (as is the case for Nyström methods), but they can also be more complex objects such as triangles or basis functions –- the only thing required is that f(X[i],Y[j]) make sense. ","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"A concrete implementation of AbstractKernelMatrix is provided by the KernelMatrix type. Creating the matrix associated with the Helmholtz free-space Greens function, for example, can be accomplished through:","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"using HMatrices, LinearAlgebra, StaticArrays\nconst Point3D = SVector{3,Float64}\nX = rand(Point3D,10_000)\nY = rand(Point3D,10_000)\nconst k = 2π\nfunction G(x,y) \n    d = norm(x-y)\n    exp(im*k*d)/(4π*d)\nend\nK = KernelMatrix(G,X,Y)","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"Compressing K is now as simple as:","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"H = assemble_hmat(K;rtol=1e-6)","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"It is worth noting that several default choices are made during the compression above. See the Assembling and HMatrix section or the documentation of assemble_hmat for information on how to obtain a more granular control of the assembling stage.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"As before, you can multiply H by a vector, or do an lu factorization of it.","category":"page"},{"location":"kernelmatrix/#Support-for-tensor-kernels","page":"Kernel matrices","title":"Support for tensor kernels","text":"","category":"section"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"warning: Warning\nSupport for tensor-valued kernels should be considered experimental at this stage.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"For vector-valued partial differential equations such as Stokes or time-harmonic Maxwell's equation, the underlying integral operator has a kernel function which is a tensor. This package currently provides some limited support for these types of operators. The example below illustrates how to build an HMatrix representing a KernelMatrix corresponding to Stokes Greens function for points on a sphere:","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"using HMatrices, LinearAlgebra, StaticArrays\nconst Point3D = SVector{3,Float64}\nm = 5_000\nX = Y = [Point3D(sin(θ)cos(ϕ),sin(θ)*sin(ϕ),cos(θ)) for (θ,ϕ) in zip(π*rand(m),2π*rand(m))]\nconst μ = 5\nfunction G(x,y)\n    r = x-y\n    d = norm(r) + 1e-10\n    1/(8π*μ) * (1/d*I + r*transpose(r)/d^3)\nend\nK = KernelMatrix(G,X,Y)\nH = assemble_hmat(K;atol=1e-4)","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"You can now multiply H by a density σ, where σ is a Vector of SVector{3,Float64}","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"σ = rand(SVector{3,Float64},m)\ny = H*σ\n# test the output agains the exact value for a given `i`\ni = 42\ny[i] - sum(K[i,j]*σ[j] for j in 1:m)","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"note: Note\nThe naive idea of reinterpreting these matrices of tensors as a (larger) matrix of scalars does not always work because care to be taken when choosing the pivot in the compression stage of the PartialACA in order to exploit some analytic properties of the underlying kernel. See e.g. section 2.3 of this paper for a brief discussion.","category":"page"},{"location":"kernelmatrix/#Vectorized-kernels-and-local-indices","page":"Kernel matrices","title":"Vectorized kernels and local indices","text":"","category":"section"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"A more efficient implementation of your kernel K::AbstractKernelMatrix can sometimes lead to faster assembling times. In particular, providing a permuted kernel Kp using the local indexing system of the HMatrix (and setting the keyword argument global_index=false in assemble_hmat) avoids frequent unnecessary index permutations, and can facilitate vectorization. This is because the permuted kernel Kp will be called through Kp[I::UnitRange,J::UnitRange] to fill in the dense blocks of the matrix, through Kp[I::UnitRange,j::int] and adjoint(Kp)[I::UnitRange,j] to build a low-rank approximation of compressible blocks. ","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"The vectorization example in the Notebook section shows how a custom (and somewhat more complex) implementation of a vectorized Laplace kernel using the LoopVectorization package can lead to faster (sequential) execution.","category":"page"}]
}
